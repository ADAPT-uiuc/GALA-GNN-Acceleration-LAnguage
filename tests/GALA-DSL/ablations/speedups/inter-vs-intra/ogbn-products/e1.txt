// Init data
G = load_dataset("Reddit");

aggrFn = dsl.get_aggregate(fn = dsl.fn.mul_sum);
L1 = layer(G, hs, nonln_fn, aggregate_fn) {
    deg = G.graphs.degrees();
    norm = dsl.fn.pow(deg, -0.5);
    res = norm * G.node.feats;
    res = aggregate_fn(G.graphs, res); // aggregate operation
    res = dsl.nn.ffn(res, out=hs); // weight operation (out hs is the output size)
    res = norm * res;
    G.node.feats = nonln_fn(res);
}

M1 = model(G, non_ln) {
	l1 = L1(G, 32, non_ln, aggrFn);
	l2 = L1(l1, G.labels.size(), non_ln, aggrFn);
}

m1 = M1(G, dsl.non_ln.ReLU);
m1.train(iters=100, validation_step=5);
res = m1.eval();

# schedule
feature_size(100); // -2 if not given
label_size(47); // -3 if not given
sparse_rewrites(false);